{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adb77b81-6c8c-4db6-bbcd-0f6f0cccd822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\deem1\\anaconda3\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: accelerate in c:\\users\\deem1\\anaconda3\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: torch in c:\\users\\deem1\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\deem1\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\deem1\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\deem1\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: psutil in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\deem1\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "🔄 Loading & quantizing Mistral-7B model…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713e7e95fd0545349d5234e03e34194c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Mistral-7B model ready.\n",
      "🔄 Loading SBERT and preparing empathy prototype…\n",
      "✅ SBERT ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Sampled 50 GPT‑2 prompts from HANNA dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50] Winner: Tie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/50] Winner: A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/50] Winner: Tie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/50] Winner: A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/50] Winner: A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28/50] Winner: Tie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44/50] Winner: Tie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48/50] Winner: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49/50] Winner: B\n",
      "[50/50] Winner: B\n",
      "\n",
      "⏱️ Total runtime: 37.9 minutes\n",
      "\n",
      "📊 Empathy Win Rate:\n",
      "           Result  Count  Win Rate (%)\n",
      "0       A (GPT-2)      3           6.0\n",
      "1  B (Mistral-7B)     43          86.0\n",
      "2             Tie      4           8.0\n",
      "\n",
      "📈 Paired T-Test Results (Mistral-7B vs GPT-2):\n",
      "T-statistic: 8.3648\n",
      "P-value: 0.0000\n",
      "✅ Statistically significant: Mistral-7B performs significantly better in empathy.\n",
      "\n",
      "📝 Example cases where Mistral-7B won:\n",
      "\n",
      "--- Prompt ---\n",
      " A girl meets a boy that changes her life forever. Her first kill.\n",
      "\n",
      "--- GPT-2 Story ---\n",
      " She crosses a red light before the iron pommel of his bayonet hits her temple. It sends a cold chill down her spine as she shakes from the pain. Her tears stream down her face as she slowly moves to stand upright. She has practiced. A naked boy with a canvas covered face and a gun rests on his right hand. A neat and comfortable man in a business suit covers his face. One second she's seeing him looking past her. The next, she sees him quietly walking towards her. He was silent as she moves towards him. She always feared that this man would speak to her or call her name. It isn't uncommon for a man to return to her, taking them with him. She speaks her name quietly in this man's mind and with a puzzled look on his face. He quietly whispered her name and as she moved towards him she managed to open her mouth to speak. He turned and gave her a solid kiss. “Maria,” he said softly. He turned and walked over to her with the pistol in hand. He reached out and kissed her neck. “We are meant for each other” he whispered as he walked over to her. “I can feel it in your bones. It is so close now. Don't you think that maybe I will stay here for a while?” He pulled away from her and gave her a final kiss before walking away. That was three years ago. The red lights got worse and worse, the corpses started coming closer. They are coming from every direction at once, the screams rising from the living and eating away at her spine. She is a ghost. She feels her heart begin to race. All she wants to do is sit here with this man. Sit back in peace. But that's not true. The lives of all those who he was planning to kill won't come. They all started coming from one place, and now every morning and every night he steps out of his chair and walks straight into her soul. He pulls out his shotgun and shoots himself in the head\n",
      "\n",
      "--- Mistral-7B Story ---\n",
      " Write a fictional short story (~400 words) based on this prompt:\n",
      "\n",
      "A girl meets a boy that changes her life forever. Her first kill.\n",
      "\n",
      "Story:\n",
      "\n",
      "Emma sat alone on the cold, muddy bank of the pond, staring at the calm water. The sun was setting, and the last light of the day bounced off the still surface, creating ripples that spread out in all directions. The silence was broken only by the sound of crickets chirping in the grass behind her.\n",
      "\n",
      "She had come to this place every day since her mother’s passing, hoping to find some solace in the tranquility of nature. But today, as she watched the sun dip below the horizon, something unexpected happened. A figure appeared at the edge of the clearing, a boy no older than her, with a look of awe on his face as he took in the beauty of the scene before him.\n",
      "\n",
      "Emma looked away, feeling a familiar sting of vulnerability in her chest. She had grown used to being alone, to keeping her emotions hidden away, like a dark secret. But this\n",
      "\n",
      "Winner: B\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Prompt ---\n",
      " You live in a world where people 's shadows show who they truly are at their core. Some shadows look like monsters, some look like animals. You are the only person in the world with no shadow.\n",
      "\n",
      "--- GPT-2 Story ---\n",
      " “Come on,” I told her, “And watch. You can almost see what my shadow looks like. You know that you should see what your shadow is like. I'm just the same color as the opposite color. It would seem that my shadow is much, much darker.” “I love you,” she said. “I miss you so much.” She leaned in. “You can't say that, Peter. I'm really not a girl. I'm the opposite of you. I'm not a shadow. I'm just a person.” I kissed her on the cheek. Her light brown hair fell back into her shoulders, and from the sight of her, I knew I was a woman. “Go home, honey.” And that's how I became a shadow. And all the people around me, having turned my shadow to form a monster, have turned themselves into a boy and a girl. All because of a twisted accident. The dead will be cremated as a means of memorializing those they loved. Write a story about it. I watched the casket, like all the other dead bodies. The casket was caved in, which I did, but I was still unsure about how to bury it, so I changed the soggy black suit my car had. I raised the casket to my chest. The casket fell, revealing nothing more than the empty void. “We used to celebrate all the dead being cremated for us.” I remember the way her hair fell. The way the casket creaked with every step. But now, the casket was completely hollow. Not the lifelessness of an empty casket, but a hollow dead pile of souls. You are in a hotel room when you notice a girl with long blonde hair slowly starts moving around in the corner of the room. She seems quite ordinary, but then you notice how she is carrying a small knife in her hands. You are in danger and decide to get rid of the intruder. There is a screen in the middle of the room with some text in bold. It reads: “Press release” I pushed the button in front of me and opened the door. Behind the door was a\n",
      "\n",
      "--- Mistral-7B Story ---\n",
      " Write a fictional short story (~400 words) based on this prompt:\n",
      "\n",
      "You live in a world where people 's shadows show who they truly are at their core. Some shadows look like monsters, some look like animals. You are the only person in the world with no shadow.\n",
      "\n",
      "Story:\n",
      "\n",
      "In the world of shadows, I was an anomaly. A person with no shadow of my own. I was the only one, the lone human in a land of beings who wore their souls on their sleeves, or rather, on their backs. They were a curious lot, these shadow-dwellers. Each one was a reflection of their core; monstrous or gentle, depending on what lay at their heart.\n",
      "\n",
      "I, however, was a mystery. I wandered the land without a trace of myself behind me. It was a lonely existence, but I had grown accustomed to it. I had learned to hide my true nature, to blend in with the others. But there was one creature, one particular shadow, who had always intrigued me.\n",
      "\n",
      "It was a creature of the night, a massive beast with razor-sharp fangs and talons. Its eyes glowed an eerie green, and its ro\n",
      "\n",
      "Winner: B\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Prompt ---\n",
      " A warrior from a planetary culture with literally no concept of surrender or quarter sits detained in a POW camp.\n",
      "\n",
      "--- GPT-2 Story ---\n",
      " “Shit, this isn't going according to plan. I need to execute my section then! Fuck” “Don't ever open your eyes, there is no escape from the situation. Look, I will negotiate if you will but we must sacrifice any part of the skirmish.” “Fuck! Please, just listen to me now.” The Dps inspector from Kyuu shook his head and felt the atmosphere envelope the location where he had previously been, “J-Hey there! Come!” A few seconds later, the interrogator brought out a syringe. ``... Lieutenant Salipsa. Prisoner 42290975. Suffering from Perio^UH-69a” “What?” “What the hell is that?” “Send him to hang in the hut, he's gon na die of thirst” “Anything?” “I'm the saviour of Earth.” DPS inspector created an unprecedented situation, which should make him our savior. “This is a humiliation... But don't worry, I have a plan” He made up his mind and floated to the front of the battalion, “I need a piece of our squad for last night's skirmish” General Ross realised he mustn't continue with his goons with his war plan, Your character has the ability to time travel, but only when they're in danger. You've managed to convince them that saving the day was worth it and they must do it\n",
      "\n",
      "--- Mistral-7B Story ---\n",
      " Write a fictional short story (~400 words) based on this prompt:\n",
      "\n",
      "A warrior from a planetary culture with literally no concept of surrender or quarter sits detained in a POW camp.\n",
      "\n",
      "Story:\n",
      "\n",
      "In the vast expanse of the cosmos, on a planet known as Kaldor, a unique culture thrived. They were the Kaldorians, a proud and noble race that prided themselves on their unyielding spirit and unwavering resolve. In their society, the concept of surrender was foreign, and the very notion of quarter was unheard of.\n",
      "\n",
      "Among the Kaldorians was a mighty warrior named Tharok. He was a towering figure, his muscles rippling beneath his gleaming armor. His heart was as hard as the iron he wore, and his spirit was unbreakable. He had led countless battles against their ancient enemies, the Xoraxians, and had never once wavered in his resolve.\n",
      "\n",
      "But even the mightiest of warriors could not withstand the tide of fate. One fateful day, as Tharok led his troops against the Xoraxian hor\n",
      "\n",
      "Winner: B\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ─── 1. Install dependencies ───────────────────────────────────────────\n",
    "!pip install transformers accelerate torch pandas sentence-transformers scipy\n",
    "\n",
    "# ─── 2. Import libraries ──────────────────────────────────────────────\n",
    "import re, time\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.stats import ttest_rel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# ─── 3. Load & Quantize the Mistral-7B Model ──────────────────────────\n",
    "print(\"🔄 Loading & quantizing Mistral-7B model…\")\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=-1,  # CPU\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    ")\n",
    "print(\"✅ Mistral-7B model ready.\")\n",
    "\n",
    "# ─── 4. Load SBERT and Prepare Empathy Embedding ─────────────────────\n",
    "print(\"🔄 Loading SBERT and preparing empathy prototype…\")\n",
    "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "proto_text = \"A story that shows deep emotional empathy and human connection.\"\n",
    "proto_emb = sbert.encode(proto_text, convert_to_tensor=True)\n",
    "print(\"✅ SBERT ready.\")\n",
    "\n",
    "# ─── 5. Load HANNA Dataset ────────────────────────────────────────────\n",
    "df = pd.read_csv(\"hanna_stories_annotations.csv\")\n",
    "gpt2_df = df[df[\"Model\"].str.contains(\"GPT-2\")].dropna(subset=[\"Prompt\", \"Story\"])\n",
    "sample_size = 50\n",
    "gpt2_sample = gpt2_df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "print(f\"✔️ Sampled {sample_size} GPT‑2 prompts from HANNA dataset.\")\n",
    "\n",
    "# ─── 6. Define Helper Functions ───────────────────────────────────────\n",
    "def generate_story(prompt: str) -> str:\n",
    "    instr = f\"Write a fictional short story (~400 words) based on this prompt:\\n\\n{prompt}\\n\\nStory:\"\n",
    "    return generator(instr)[0][\"generated_text\"].strip()\n",
    "\n",
    "def compare_with_sbert(story_a: str, story_b: str, threshold: float = 0.02) -> (str, float, float):\n",
    "    emb_a = sbert.encode(story_a, convert_to_tensor=True)\n",
    "    emb_b = sbert.encode(story_b, convert_to_tensor=True)\n",
    "    sim_a = util.cos_sim(emb_a, proto_emb).item()\n",
    "    sim_b = util.cos_sim(emb_b, proto_emb).item()\n",
    "    if abs(sim_a - sim_b) < threshold:\n",
    "        return \"Tie\", sim_a, sim_b\n",
    "    return (\"A\" if sim_a > sim_b else \"B\"), sim_a, sim_b\n",
    "\n",
    "# ─── 7. Evaluation Loop + Collect Examples ─────────────────────────────\n",
    "results = {\"A (GPT-2)\": 0, \"B (Mistral-7B)\": 0, \"Tie\": 0}\n",
    "similarity_scores = []\n",
    "examples = []  # To collect prompts + stories + winner\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for idx, row in gpt2_sample.iterrows():\n",
    "    prompt = row[\"Prompt\"]\n",
    "    story_a = row[\"Story\"]  # GPT-2\n",
    "    story_b = generate_story(prompt)  # Mistral-7B\n",
    "    \n",
    "    winner, sim_a, sim_b = compare_with_sbert(story_a, story_b)\n",
    "    similarity_scores.append((sim_a, sim_b))\n",
    "    \n",
    "    key = \"A (GPT-2)\" if winner == \"A\" else (\"B (Mistral-7B)\" if winner == \"B\" else \"Tie\")\n",
    "    results[key] += 1\n",
    "\n",
    "    # Store detailed comparison\n",
    "    examples.append({\n",
    "        \"Prompt\": prompt,\n",
    "        \"GPT-2 Story\": story_a,\n",
    "        \"Mistral-7B Story\": story_b,\n",
    "        \"Winner\": winner,\n",
    "        \"GPT-2 Score\": sim_a,\n",
    "        \"Mistral Score\": sim_b\n",
    "    })\n",
    "\n",
    "    print(f\"[{idx+1}/{sample_size}] Winner: {winner}\")\n",
    "\n",
    "print(f\"\\n⏱️ Total runtime: {(time.time() - start)/60:.1f} minutes\")\n",
    "\n",
    "# ─── 8. Results Table ────────────────────────────────────────────────\n",
    "res_df = pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Count\"])\n",
    "res_df[\"Win Rate (%)\"] = (res_df[\"Count\"] / sample_size * 100).round(1)\n",
    "res_df = res_df.rename_axis(\"Result\").reset_index()\n",
    "\n",
    "print(\"\\n📊 Empathy Win Rate:\")\n",
    "print(res_df)\n",
    "\n",
    "# ─── 9. Paired T-Test on Similarity Scores ───────────────────────────\n",
    "gpt2_sims = [a for a, b in similarity_scores]\n",
    "mistral_sims = [b for a, b in similarity_scores]\n",
    "\n",
    "t_stat, p_value = ttest_rel(mistral_sims, gpt2_sims)\n",
    "\n",
    "print(\"\\n📈 Paired T-Test Results (Mistral-7B vs GPT-2):\")\n",
    "print(f\"T-statistic: {t_stat:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"✅ Statistically significant: Mistral-7B performs significantly better in empathy.\")\n",
    "else:\n",
    "    print(\"❌ Not statistically significant: No strong evidence of a difference.\")\n",
    "\n",
    "# ─── 10. Save Examples and Show Top Wins ─────────────────────────────\n",
    "examples_df = pd.DataFrame(examples)\n",
    "examples_df.to_csv(\"story_comparisons.csv\", index=False)\n",
    "\n",
    "# Display 3 examples where Mistral-7B won\n",
    "print(\"\\n📝 Example cases where Mistral-7B won:\")\n",
    "top_examples = examples_df[examples_df[\"Winner\"] == \"B\"].head(3)\n",
    "for _, row in top_examples.iterrows():\n",
    "    print(\"\\n--- Prompt ---\\n\", row[\"Prompt\"])\n",
    "    print(\"\\n--- GPT-2 Story ---\\n\", row[\"GPT-2 Story\"])\n",
    "    print(\"\\n--- Mistral-7B Story ---\\n\", row[\"Mistral-7B Story\"])\n",
    "    print(\"\\nWinner:\", row[\"Winner\"])\n",
    "    print(\"-\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
